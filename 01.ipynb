{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4640f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9c4b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, in_features=100, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=100)\n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=50)\n",
    "        self.output_layer = nn.Linear(in_features=50, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc2(x1)\n",
    "        return self.output_layer(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "650e1b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel2                                 [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 10]                   --\n",
       "│    └─Linear: 2-1                       [1, 100]                  10,100\n",
       "│    └─ReLU: 2-2                         [1, 100]                  --\n",
       "│    └─Linear: 2-3                       [1, 50]                   5,050\n",
       "│    └─ReLU: 2-4                         [1, 50]                   --\n",
       "│    └─Linear: 2-5                       [1, 10]                   510\n",
       "==========================================================================================\n",
       "Total params: 15,660\n",
       "Trainable params: 15,660\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.06\n",
       "Estimated Total Size (MB): 0.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model2, (1, 100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a4ab378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "mnist = torchvision.datasets.MNIST(root='.', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3eaf0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = mnist._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7625f857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([60000]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0d4dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9456a071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor(8)')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "idx = random.randint(0, x.shape[0])\n",
    "\n",
    "plt.imshow(x[idx])\n",
    "plt.title(y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdd05c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46863477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ced0b228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28 *28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b8d1656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0196,  0.0038, -0.0165,  ...,  0.0186,  0.0190,  0.0238],\n",
       "         [-0.0114,  0.0348,  0.0343,  ..., -0.0113,  0.0220, -0.0101],\n",
       "         [ 0.0241, -0.0062,  0.0277,  ...,  0.0168, -0.0253, -0.0054],\n",
       "         ...,\n",
       "         [ 0.0177, -0.0320, -0.0022,  ...,  0.0147,  0.0138, -0.0057],\n",
       "         [-0.0082,  0.0023,  0.0157,  ..., -0.0247, -0.0087, -0.0057],\n",
       "         [-0.0289,  0.0269, -0.0085,  ..., -0.0281,  0.0096, -0.0134]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0350,  0.0231, -0.0082, -0.0103, -0.0203,  0.0226,  0.0348,  0.0146,\n",
       "          0.0034, -0.0207,  0.0232, -0.0033, -0.0166,  0.0136,  0.0070, -0.0331,\n",
       "          0.0093, -0.0046,  0.0092, -0.0250, -0.0014,  0.0254,  0.0092,  0.0043,\n",
       "          0.0209, -0.0098,  0.0022, -0.0125, -0.0091,  0.0282, -0.0312, -0.0297,\n",
       "         -0.0342,  0.0175,  0.0104,  0.0264, -0.0159, -0.0090,  0.0105, -0.0285,\n",
       "         -0.0005,  0.0025, -0.0233, -0.0058,  0.0199,  0.0082, -0.0125, -0.0057,\n",
       "          0.0268,  0.0050,  0.0016,  0.0256,  0.0232, -0.0349,  0.0097,  0.0083,\n",
       "          0.0164, -0.0339,  0.0340,  0.0083,  0.0100, -0.0214,  0.0019, -0.0331,\n",
       "         -0.0155,  0.0207, -0.0065, -0.0315,  0.0011,  0.0188,  0.0068,  0.0097,\n",
       "         -0.0011, -0.0306, -0.0262, -0.0214,  0.0215,  0.0039, -0.0091, -0.0236,\n",
       "         -0.0159, -0.0313,  0.0268,  0.0036, -0.0058,  0.0218,  0.0154, -0.0343,\n",
       "          0.0324, -0.0343, -0.0127,  0.0057,  0.0339,  0.0182, -0.0154,  0.0025,\n",
       "         -0.0021, -0.0243,  0.0071, -0.0103], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0075, -0.0133,  0.0153,  ...,  0.0697, -0.0412, -0.0113],\n",
       "         [-0.0739, -0.0248,  0.0548,  ...,  0.0860,  0.0498,  0.0013],\n",
       "         [-0.0423,  0.0633, -0.0902,  ...,  0.0530, -0.0114,  0.0860],\n",
       "         ...,\n",
       "         [-0.0090, -0.0117, -0.0830,  ..., -0.0803, -0.0265,  0.0403],\n",
       "         [ 0.0626,  0.0991,  0.0048,  ...,  0.0297,  0.0277, -0.0280],\n",
       "         [-0.0648, -0.0906, -0.0786,  ...,  0.0055, -0.0548,  0.0793]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0409,  0.0455, -0.0181, -0.0519, -0.0918, -0.0099, -0.0557,  0.0156,\n",
       "          0.0699,  0.0712,  0.0687, -0.0625,  0.0556, -0.0230, -0.0892, -0.0034,\n",
       "          0.0036,  0.0568, -0.0973, -0.0133, -0.0040, -0.0170,  0.0668,  0.0124,\n",
       "          0.0745,  0.0036, -0.0815, -0.0442,  0.0152, -0.0369,  0.0583, -0.0294,\n",
       "          0.0726, -0.0706, -0.0097, -0.0892, -0.0364,  0.0758,  0.0122, -0.0499,\n",
       "          0.0416, -0.0714, -0.0026,  0.0496,  0.0325,  0.0340,  0.0140,  0.0177,\n",
       "          0.0395,  0.0631], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 6.4403e-02, -7.9374e-02,  9.6339e-02, -5.9243e-02,  5.6515e-03,\n",
       "          -2.7832e-02, -6.3038e-02,  3.0042e-05,  6.1039e-02, -5.4840e-02,\n",
       "          -1.3923e-01, -3.6409e-03,  1.2052e-01, -1.3481e-01, -2.4814e-02,\n",
       "           1.3648e-01, -4.0834e-02, -1.1004e-01, -1.2222e-01, -5.5128e-02,\n",
       "          -1.8307e-02,  1.7242e-02,  1.2421e-01, -6.9218e-03, -1.2033e-01,\n",
       "           6.4811e-02, -6.3572e-02,  1.1468e-01, -3.1475e-02, -1.2492e-01,\n",
       "           6.8593e-02, -1.0848e-01, -1.3152e-01,  1.1832e-01, -3.5471e-02,\n",
       "          -1.3953e-01,  4.5400e-02, -5.4759e-02,  1.2580e-01,  1.0383e-01,\n",
       "          -7.4526e-02,  9.7381e-02, -1.3500e-01,  1.1750e-01, -1.0748e-02,\n",
       "           5.1462e-02,  1.2454e-01,  8.2831e-02, -4.2867e-02,  1.1729e-01],\n",
       "         [ 1.3594e-01,  4.6288e-02, -1.7481e-02,  4.1220e-02, -9.0962e-02,\n",
       "          -5.8924e-02,  3.0633e-02, -9.6028e-02,  7.9956e-02, -1.4006e-01,\n",
       "           1.2635e-01,  8.8705e-02, -1.0781e-02, -1.2524e-01,  6.8804e-02,\n",
       "          -7.9466e-02, -2.4629e-02,  1.2249e-01,  1.8173e-02,  2.4164e-02,\n",
       "           7.3244e-02, -1.0496e-01, -5.0155e-02, -4.7876e-02,  3.5711e-02,\n",
       "           3.8737e-02,  6.2014e-02,  1.1759e-01,  1.1265e-01,  1.1860e-01,\n",
       "           6.0705e-02,  2.1271e-02, -7.2198e-02, -4.2513e-02,  2.1512e-02,\n",
       "          -1.2334e-01,  8.3744e-02,  7.7261e-02, -1.1454e-01, -1.1475e-01,\n",
       "          -3.2805e-02, -2.6968e-02,  1.3298e-01,  3.7719e-02,  7.0771e-02,\n",
       "           1.2915e-01,  8.1956e-02,  4.9800e-02, -1.0238e-01, -4.4444e-02],\n",
       "         [ 9.6094e-02,  9.1366e-03, -8.3107e-03,  3.8557e-02, -9.3324e-02,\n",
       "           1.1374e-01,  5.1728e-02, -1.0807e-01, -1.2014e-01, -6.0344e-02,\n",
       "           1.3986e-01, -6.5640e-02,  1.1526e-01,  7.9403e-02, -3.0369e-02,\n",
       "           9.5559e-02,  6.2676e-02, -8.4988e-02,  1.0199e-01, -5.0942e-02,\n",
       "           4.7333e-02,  5.4959e-02,  9.9363e-02,  1.3918e-01,  1.1360e-01,\n",
       "          -9.5127e-03,  9.2242e-03, -1.0098e-01,  8.2677e-02, -1.8819e-02,\n",
       "          -7.5620e-02, -1.1927e-01,  3.5222e-02, -7.0514e-02,  9.2527e-02,\n",
       "          -6.0586e-02, -1.1966e-01, -8.2993e-02, -1.4954e-03, -9.8629e-03,\n",
       "           1.2118e-01, -9.2118e-02,  2.8903e-02, -1.0154e-01, -1.4045e-02,\n",
       "           1.0419e-01, -9.8727e-02,  7.6170e-03,  2.5421e-02, -5.9412e-03],\n",
       "         [ 3.5495e-02, -9.0133e-02, -8.6461e-02, -2.0177e-02, -1.0122e-01,\n",
       "           4.0786e-02, -9.9467e-03, -2.5914e-02, -4.9839e-02, -1.1627e-01,\n",
       "          -3.4617e-02, -1.1911e-01, -3.6240e-02, -8.1488e-04, -5.5133e-02,\n",
       "           3.1984e-02, -9.1240e-02,  1.0139e-01, -5.7094e-02, -2.8226e-03,\n",
       "           1.0743e-01, -1.3074e-01, -1.9758e-02, -1.3201e-01, -1.0275e-01,\n",
       "           4.0850e-02, -3.3558e-02,  5.7672e-02, -1.2960e-01,  1.1767e-01,\n",
       "           2.3624e-02, -8.5612e-02,  3.5563e-02, -1.2055e-01, -7.3141e-02,\n",
       "           1.1975e-01,  8.7177e-02, -1.1101e-01, -1.2346e-01, -2.9013e-02,\n",
       "           3.5631e-02,  7.0888e-02, -1.3489e-02, -8.0860e-02,  7.3004e-02,\n",
       "          -3.4909e-02, -7.5857e-02,  3.4358e-02,  1.5004e-02, -7.8548e-02],\n",
       "         [ 4.0233e-02,  1.1598e-01,  7.9662e-02,  1.0755e-01,  1.3336e-02,\n",
       "           6.7107e-02,  1.1535e-01,  3.2742e-02,  1.2069e-02,  6.2960e-02,\n",
       "           5.0549e-02,  2.9427e-02,  5.2289e-02,  1.4091e-01,  5.7781e-02,\n",
       "           6.0998e-02, -8.7422e-02, -1.2700e-02,  9.1490e-02,  8.2918e-02,\n",
       "           4.9774e-02, -5.0252e-02, -5.4268e-03,  1.3752e-01,  2.3053e-02,\n",
       "           5.4779e-02,  8.7403e-02,  5.7064e-02, -3.0258e-02, -1.3263e-01,\n",
       "           1.1155e-02, -9.9097e-02, -3.0232e-02, -6.9699e-02,  2.4102e-02,\n",
       "          -5.7242e-03, -3.4738e-02, -1.0590e-01, -2.6960e-02,  6.7297e-02,\n",
       "           3.7939e-02,  9.3972e-02, -6.9465e-03,  4.0921e-02, -1.0649e-01,\n",
       "           5.0780e-02, -2.3956e-02, -1.3775e-01,  1.9202e-02,  8.3819e-02],\n",
       "         [ 8.4308e-02,  8.6617e-02, -4.5942e-02, -1.2469e-01,  3.9554e-03,\n",
       "           6.8373e-03, -5.4379e-02, -4.5410e-02, -1.2478e-01,  6.8600e-02,\n",
       "          -6.3954e-02, -1.8085e-02, -6.1845e-02,  2.7397e-02,  1.3551e-01,\n",
       "           6.8794e-02, -1.1777e-01,  6.9318e-02, -1.4398e-02,  6.0108e-02,\n",
       "          -1.0645e-01, -1.3670e-01, -5.7227e-02, -4.3320e-02, -7.6441e-02,\n",
       "          -8.0768e-03,  1.3059e-01,  2.1461e-02, -8.8351e-02,  1.1823e-01,\n",
       "          -4.1692e-02,  5.7250e-02, -2.5298e-02,  4.5366e-02,  3.2864e-02,\n",
       "           2.8121e-02,  2.1408e-02,  1.3763e-01,  1.2523e-01,  4.3602e-03,\n",
       "          -4.3461e-03, -9.6415e-04, -7.3924e-02,  2.9744e-02,  1.3562e-01,\n",
       "          -9.7693e-02, -8.6692e-03,  5.0717e-02,  8.3495e-03,  8.8387e-02],\n",
       "         [ 7.2299e-02,  4.2223e-02, -6.0265e-02,  4.5400e-03,  1.0894e-03,\n",
       "          -9.5439e-02, -4.9832e-02,  1.1397e-01, -5.7889e-02, -3.5272e-02,\n",
       "           1.4099e-01,  1.0974e-01, -3.7836e-02, -8.8597e-02, -1.0656e-02,\n",
       "           3.5272e-02, -1.2024e-01, -5.3397e-02, -3.8192e-02,  1.2423e-01,\n",
       "           7.9370e-02, -6.0380e-02,  1.2727e-01,  1.2463e-01, -4.9209e-02,\n",
       "           7.2721e-03,  4.7052e-02, -1.1847e-02, -1.3617e-01, -1.4368e-02,\n",
       "          -1.9240e-02, -9.1721e-02,  6.7095e-03,  4.2477e-02,  1.0982e-02,\n",
       "          -9.2564e-02,  6.6852e-03,  7.9657e-02,  1.3209e-01, -1.2009e-01,\n",
       "          -8.8677e-02,  1.3603e-01, -3.4733e-02, -1.0281e-01, -5.4802e-03,\n",
       "           4.3034e-02, -7.7444e-02,  1.1748e-01, -5.9894e-02,  3.3661e-03],\n",
       "         [ 1.3367e-01, -9.7474e-02, -3.0786e-02,  3.8697e-02,  9.3753e-02,\n",
       "           4.2291e-02,  1.0264e-01, -1.0102e-01,  1.4085e-01, -6.5901e-02,\n",
       "          -1.3186e-03,  7.8188e-02, -3.9104e-02, -1.3779e-01, -1.0643e-01,\n",
       "           9.4536e-02,  5.6454e-02, -4.7614e-02,  4.5023e-02,  4.7304e-02,\n",
       "           4.6363e-02, -1.3400e-01, -1.0594e-01, -1.1642e-01,  2.8185e-03,\n",
       "           6.5983e-02,  1.1723e-01, -2.4212e-02,  3.9766e-02, -1.3592e-01,\n",
       "          -1.3251e-01, -8.7547e-02, -8.2075e-02, -1.0567e-01, -7.8822e-02,\n",
       "           6.3108e-02,  1.7724e-03, -8.6685e-02, -1.2966e-01,  3.5205e-02,\n",
       "          -8.6240e-02, -7.0652e-02, -7.1788e-02, -1.6386e-02,  6.2707e-02,\n",
       "          -7.4335e-02,  4.3472e-02,  1.2800e-01,  9.2056e-03, -4.3638e-02],\n",
       "         [ 1.3905e-01,  1.4011e-01,  9.5342e-02, -1.0859e-02, -1.2177e-01,\n",
       "          -1.3866e-01, -9.2958e-02, -1.2675e-01, -4.4866e-02, -1.1531e-01,\n",
       "          -2.4156e-02,  4.4988e-02,  5.2576e-04, -4.5297e-02,  7.8227e-02,\n",
       "          -8.2281e-02,  8.3113e-02,  6.6237e-02, -1.5579e-03, -3.4627e-02,\n",
       "          -4.0579e-02,  6.6491e-02, -1.2541e-01, -9.3732e-02,  6.7265e-02,\n",
       "          -1.1060e-01,  1.3324e-01, -1.0360e-01,  4.0095e-02, -6.8152e-02,\n",
       "           2.6765e-02,  1.2560e-02, -1.0061e-02,  2.4627e-02,  1.2661e-02,\n",
       "          -6.2363e-03,  4.5263e-02, -8.5221e-02,  1.2630e-01,  6.6048e-02,\n",
       "          -2.1905e-02,  4.8565e-02, -7.6438e-02,  1.3836e-02,  3.9135e-02,\n",
       "           7.2309e-02, -7.4198e-02,  3.6116e-02, -9.5682e-02,  1.3452e-02],\n",
       "         [ 1.0253e-01,  1.0646e-01, -1.0183e-01,  7.0451e-02,  1.5281e-02,\n",
       "          -8.3469e-02, -1.3873e-01, -5.8427e-02, -6.9852e-02,  6.6757e-02,\n",
       "          -9.6363e-03,  6.3788e-02, -8.4469e-02, -9.9824e-02,  8.1014e-03,\n",
       "          -9.6302e-02, -4.3289e-02, -6.6900e-02,  7.8707e-02,  1.1482e-03,\n",
       "           7.5248e-02,  1.8157e-02,  1.0965e-01,  9.5087e-02, -4.6400e-02,\n",
       "          -1.2636e-01,  4.9512e-02,  4.8061e-02,  7.1183e-02, -1.2823e-01,\n",
       "           3.3251e-02, -1.8885e-02,  9.1188e-02, -3.3978e-02, -5.4082e-02,\n",
       "           8.3085e-02,  8.8356e-03, -8.5251e-02, -5.5379e-02, -6.7161e-02,\n",
       "          -9.7447e-02, -9.3786e-02, -9.5586e-02,  9.4529e-02, -8.2847e-02,\n",
       "           1.1668e-01,  8.2846e-02, -3.4776e-02, -1.2949e-02, -7.5888e-02]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1015,  0.0670,  0.0167, -0.1218, -0.0085, -0.0392,  0.1270, -0.0317,\n",
       "          0.0985, -0.0399], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel2(nn.Module):\n",
    "    def __init__(self, in_features=100, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=in_features, out_features=100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=100, out_features=50),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=50, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "model = MyModel2(in_features=784).to('cuda')\n",
    "list(model.parameters())\n",
    "# summary(model, (32, (28, 28),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "497903da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([50, 100])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for parameters in model.parameters():\n",
    "    print(parameters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1f138232",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = x[0].type(torch.float32).to('cuda')\n",
    "xa = xa.unsqueeze(0)\n",
    "# xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "acc9fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.5520,  -1.3612,  -0.9038,  31.0500, -22.8875,  37.3098,  14.6757,\n",
       "         -19.3566,   8.6605,  -0.7909]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e29c5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1.5520,  -1.3612,  -0.9038,  31.0500, -22.8875,  37.3098,  14.6757,\n",
      "         -19.3566,   8.6605,  -0.7909]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    print(model(xa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873dc09",
   "metadata": {},
   "source": [
    "1. Model                                        X\n",
    "2. Training Data                                X\n",
    "3. Evaluation Data (Validaion data)             X\n",
    "4. Evaluation Metrics                           []\n",
    "5. Optimizer                                    X\n",
    "6. Loss Function                                X\n",
    "7. Training Loop\n",
    "    - Training\n",
    "    - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c1c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000, 28, 28]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = torchvision.datasets.MNIST(root='.', download=True, train=True)._load_data()\n",
    "test_x, test_y = torchvision.datasets.MNIST(root='.', download=True, train=False)._load_data()\n",
    "\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ff8af7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.uint8, torch.int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.dtype, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e39ce83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[:10000].type(torch.float32)\n",
    "train_y = train_y[:10000].type(torch.LongTensor)\n",
    "\n",
    "test_x = test_x.type(torch.float32)\n",
    "test_y = test_y.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3402b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel2(in_features=784).to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4fa031f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.75"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Epoch 1/10\n",
      "0.042135272175073624\n",
      "0.0821027159690857\n",
      "0.019203148782253265\n",
      "0.0452568493783474\n",
      "0.01947234757244587\n",
      "0.0685286819934845\n",
      "0.029838455840945244\n",
      "0.06053156778216362\n",
      "0.042169421911239624\n",
      "0.10254160314798355\n",
      "0.05106246843934059\n",
      "0.01597290299832821\n",
      "0.0770055428147316\n",
      "0.02155301161110401\n",
      "0.050034601241350174\n",
      "0.1258963793516159\n",
      "0.021164502948522568\n",
      "0.04429895058274269\n",
      "0.03302150219678879\n",
      "0.014018157497048378\n",
      "0.02627362124621868\n",
      "0.017805838957428932\n",
      "0.02417607419192791\n",
      "0.01621570810675621\n",
      "0.030730420723557472\n",
      "0.05374198779463768\n",
      "0.01992550678551197\n",
      "0.06485239416360855\n",
      "0.04019809141755104\n",
      "0.03909135237336159\n",
      "0.011134200729429722\n",
      "0.011058266274631023\n",
      "0.031411606818437576\n",
      "0.010303376242518425\n",
      "0.0510086789727211\n",
      "0.024050306528806686\n",
      "0.04568205401301384\n",
      "0.04680899530649185\n",
      "0.022493690252304077\n",
      "0.08162742108106613\n",
      "0.04195510968565941\n",
      "0.02054605633020401\n",
      "0.02347537875175476\n",
      "0.08523828536272049\n",
      "0.05295921117067337\n",
      "0.09771335870027542\n",
      "0.019163792952895164\n",
      "0.043286580592393875\n",
      "0.038985248655080795\n",
      "0.02609195001423359\n",
      "0.011839056387543678\n",
      "0.013262861408293247\n",
      "0.02049056813120842\n",
      "0.06114189699292183\n",
      "0.10116592049598694\n",
      "0.018087780103087425\n",
      "0.02659814991056919\n",
      "0.0381862074136734\n",
      "0.017601193860173225\n",
      "0.024259306490421295\n",
      "0.1196327954530716\n",
      "0.017059486359357834\n",
      "0.05111115798354149\n",
      "0.0689256563782692\n",
      "0.08187203109264374\n",
      "0.027184344828128815\n",
      "0.05734386295080185\n",
      "0.05862325802445412\n",
      "0.09101946651935577\n",
      "0.07403429597616196\n",
      "0.016357241198420525\n",
      "0.013017021119594574\n",
      "0.09130213409662247\n",
      "0.02371492050588131\n",
      "0.012054306454956532\n",
      "0.004728284664452076\n",
      "0.03324045240879059\n",
      "0.016133125871419907\n",
      "0.001147098490037024\n",
      "Train Loss: 0.0415 | Val Loss: 0.3230\n",
      "Epoch 2/10\n",
      "0.014645486138761044\n",
      "0.058737754821777344\n",
      "0.012599771842360497\n",
      "0.01685689203441143\n",
      "0.04537211358547211\n",
      "0.0059349751099944115\n",
      "0.018889078870415688\n",
      "0.012431480921804905\n",
      "0.04489007592201233\n",
      "0.01744268275797367\n",
      "0.02896646410226822\n",
      "0.04211483523249626\n",
      "0.050793103873729706\n",
      "0.006282255984842777\n",
      "0.009494990110397339\n",
      "0.048646554350852966\n",
      "0.0320635549724102\n",
      "0.040119390934705734\n",
      "0.021297672763466835\n",
      "0.009942799806594849\n",
      "0.031184272840619087\n",
      "0.024935254827141762\n",
      "0.00927227083593607\n",
      "0.028124621137976646\n",
      "0.019363561645150185\n",
      "0.012425337918102741\n",
      "0.01808335818350315\n",
      "0.0662756934762001\n",
      "0.04258529841899872\n",
      "0.008997627533972263\n",
      "0.004681338090449572\n",
      "0.013174396939575672\n",
      "0.014819048345088959\n",
      "0.017878787592053413\n",
      "0.060057174414396286\n",
      "0.02809979021549225\n",
      "0.04785783588886261\n",
      "0.009085327386856079\n",
      "0.05402221903204918\n",
      "0.03574578836560249\n",
      "0.017552051693201065\n",
      "0.01617763563990593\n",
      "0.007187875919044018\n",
      "0.02089783176779747\n",
      "0.010073298588395119\n",
      "0.024022212252020836\n",
      "0.015755152329802513\n",
      "0.022676926106214523\n",
      "0.006293362006545067\n",
      "0.013602950610220432\n",
      "0.03908154368400574\n",
      "0.005951777566224337\n",
      "0.012120855040848255\n",
      "0.14333927631378174\n",
      "0.01052230503410101\n",
      "0.021160701289772987\n",
      "0.022030750289559364\n",
      "0.02583652175962925\n",
      "0.006448422092944384\n",
      "0.019160088151693344\n",
      "0.020575273782014847\n",
      "0.026442838832736015\n",
      "0.0031857057474553585\n",
      "0.041354089975357056\n",
      "0.05720055103302002\n",
      "0.006746381521224976\n",
      "0.011559151113033295\n",
      "0.01902134157717228\n",
      "0.024617692455649376\n",
      "0.03900253027677536\n",
      "0.057328447699546814\n",
      "0.005839123856276274\n",
      "0.04239286854863167\n",
      "0.008474418893456459\n",
      "0.010412875562906265\n",
      "0.0028206275310367346\n",
      "0.016865350306034088\n",
      "0.01676018163561821\n",
      "0.00026518036611378193\n",
      "Train Loss: 0.0247 | Val Loss: 0.3419\n",
      "Epoch 3/10\n",
      "0.02073841355741024\n",
      "0.04489666223526001\n",
      "0.007407129276543856\n",
      "0.017654668539762497\n",
      "0.012388249859213829\n",
      "0.02466139756143093\n",
      "0.029946066439151764\n",
      "0.02657278999686241\n",
      "0.06282936036586761\n",
      "0.0343867726624012\n",
      "0.016766533255577087\n",
      "0.022420769557356834\n",
      "0.05172214284539223\n",
      "0.014502760022878647\n",
      "0.0165671668946743\n",
      "0.020669030025601387\n",
      "0.004033513367176056\n",
      "0.01720411516726017\n",
      "0.0022465079091489315\n",
      "0.016835330054163933\n",
      "0.0035397158935666084\n",
      "0.0354805663228035\n",
      "0.017080824822187424\n",
      "0.03677121177315712\n",
      "0.02187010459601879\n",
      "0.026195403188467026\n",
      "0.004864245653152466\n",
      "0.008889804594218731\n",
      "0.08359983563423157\n",
      "0.0751771330833435\n",
      "0.01594836264848709\n",
      "0.0050039347261190414\n",
      "0.0024084721226245165\n",
      "0.005868941079825163\n",
      "0.014046155847609043\n",
      "0.01604972407221794\n",
      "0.057553231716156006\n",
      "0.018019279465079308\n",
      "0.042102206498384476\n",
      "0.02127031795680523\n",
      "0.024656109511852264\n",
      "0.047820933163166046\n",
      "0.02426893450319767\n",
      "0.02936193160712719\n",
      "0.0671541839838028\n",
      "0.03886553272604942\n",
      "0.016562826931476593\n",
      "0.006098636891692877\n",
      "0.03327355906367302\n",
      "0.024182166904211044\n",
      "0.015091649256646633\n",
      "0.007293411530554295\n",
      "0.013451206497848034\n",
      "0.09135616570711136\n",
      "0.016818540170788765\n",
      "0.03434755653142929\n",
      "0.020545635372400284\n",
      "0.007593784015625715\n",
      "0.003921537194401026\n",
      "0.07820168137550354\n",
      "0.015070690773427486\n",
      "0.019227491691708565\n",
      "0.00504380464553833\n",
      "0.027876410633325577\n",
      "0.028024928644299507\n",
      "0.01680123247206211\n",
      "0.019667984917759895\n",
      "0.07344485074281693\n",
      "0.00877451803535223\n",
      "0.008899839594960213\n",
      "0.025496650487184525\n",
      "0.00534325884655118\n",
      "0.05601755902171135\n",
      "0.06710723042488098\n",
      "0.018962185829877853\n",
      "0.003020914737135172\n",
      "0.018782315775752068\n",
      "0.0026600381825119257\n",
      "0.090936079621315\n",
      "Train Loss: 0.0264 | Val Loss: 0.3349\n",
      "Epoch 4/10\n",
      "0.012807508930563927\n",
      "0.09102896600961685\n",
      "0.01403981912881136\n",
      "0.11955367773771286\n",
      "0.1686287224292755\n",
      "0.014499636366963387\n",
      "0.21197199821472168\n",
      "0.15758973360061646\n",
      "0.09429015964269638\n",
      "0.40111076831817627\n",
      "0.25498607754707336\n",
      "0.016620825976133347\n",
      "0.03853536769747734\n",
      "0.03191039711236954\n",
      "0.0634179338812828\n",
      "0.2226664125919342\n",
      "0.14858245849609375\n",
      "0.0938369631767273\n",
      "0.03194097429513931\n",
      "0.10774584114551544\n",
      "0.14271411299705505\n",
      "0.04381134733557701\n",
      "0.07730738073587418\n",
      "0.13004854321479797\n",
      "0.08682742714881897\n",
      "0.09693817049264908\n",
      "0.04361788183450699\n",
      "0.02446587011218071\n",
      "0.06622909754514694\n",
      "0.18578876554965973\n",
      "0.009049171581864357\n",
      "0.0662083625793457\n",
      "0.17091931402683258\n",
      "0.053110670298337936\n",
      "0.13815937936306\n",
      "0.03369564190506935\n",
      "0.051437776535749435\n",
      "0.066672183573246\n",
      "0.09660640358924866\n",
      "0.013620439916849136\n",
      "0.15480570495128632\n",
      "0.15420083701610565\n",
      "0.03032051771879196\n",
      "0.07759265601634979\n",
      "0.013534639962017536\n",
      "0.03615712374448776\n",
      "0.006018131505697966\n",
      "0.04005375877022743\n",
      "0.1258874386548996\n",
      "0.09223281592130661\n",
      "0.04503936320543289\n",
      "0.019070815294981003\n",
      "0.14745590090751648\n",
      "0.18063701689243317\n",
      "0.15219202637672424\n",
      "0.06642568856477737\n",
      "0.11790771782398224\n",
      "0.044883351773023605\n",
      "0.046853478997945786\n",
      "0.11285517364740372\n",
      "0.2600671052932739\n",
      "0.050130974501371384\n",
      "0.0937260240316391\n",
      "0.06750093400478363\n",
      "0.19780057668685913\n",
      "0.031363360583782196\n",
      "0.10151824355125427\n",
      "0.07566480338573456\n",
      "0.06845390796661377\n",
      "0.11220423132181168\n",
      "0.022915873676538467\n",
      "0.044992584735155106\n",
      "0.045593321323394775\n",
      "0.06654748320579529\n",
      "0.049689967185258865\n",
      "0.007591739296913147\n",
      "0.06615549325942993\n",
      "0.044618938118219376\n",
      "9.604954539099708e-05\n",
      "Train Loss: 0.0881 | Val Loss: 0.3990\n",
      "Epoch 5/10\n",
      "0.04265422374010086\n",
      "0.039995428174734116\n",
      "0.037952765822410583\n",
      "0.016819192096590996\n",
      "0.14966638386249542\n",
      "0.07397404313087463\n",
      "0.1460016965866089\n",
      "0.16374428570270538\n",
      "0.18286338448524475\n",
      "0.09256914258003235\n",
      "0.1631934493780136\n",
      "0.024783944711089134\n",
      "0.06506182998418808\n",
      "0.03221224248409271\n",
      "0.05610773712396622\n",
      "0.11835216730833054\n",
      "0.01382810901850462\n",
      "0.027189237996935844\n",
      "0.05496831238269806\n",
      "0.01773204281926155\n",
      "0.015260408632457256\n",
      "0.05652609467506409\n",
      "0.10806607455015182\n",
      "0.03472912684082985\n",
      "0.02266266942024231\n",
      "0.020167039707303047\n",
      "0.007791072595864534\n",
      "0.011485026217997074\n",
      "0.06974313408136368\n",
      "0.15065450966358185\n",
      "0.04177847504615784\n",
      "0.15619929134845734\n",
      "0.027806274592876434\n",
      "0.009385458193719387\n",
      "0.013945106416940689\n",
      "0.012461521662771702\n",
      "0.1124408096075058\n",
      "0.10148674994707108\n",
      "0.06874926388263702\n",
      "0.05531683936715126\n",
      "0.046295780688524246\n",
      "0.05161125585436821\n",
      "0.0164326224476099\n",
      "0.05912763625383377\n",
      "0.14571167528629303\n",
      "0.052668604999780655\n",
      "0.01717250607907772\n",
      "0.03478936105966568\n",
      "0.020171917974948883\n",
      "0.017029965296387672\n",
      "0.10998258739709854\n",
      "0.018403053283691406\n",
      "0.03475788235664368\n",
      "0.161957249045372\n",
      "0.06815343350172043\n",
      "0.029782403260469437\n",
      "0.023333530873060226\n",
      "0.19112300872802734\n",
      "0.018249811604619026\n",
      "0.05126556381583214\n",
      "0.03847594931721687\n",
      "0.03708449378609657\n",
      "0.02156716212630272\n",
      "0.04593593254685402\n",
      "0.07935907691717148\n",
      "0.042981039732694626\n",
      "0.033419858664274216\n",
      "0.11685634404420853\n",
      "0.05563182011246681\n",
      "0.07856492698192596\n",
      "0.043624114245176315\n",
      "0.02162138745188713\n",
      "0.06309976428747177\n",
      "0.0366777740418911\n",
      "0.020146863535046577\n",
      "0.023424752056598663\n",
      "0.01958337053656578\n",
      "0.012600179761648178\n",
      "0.04014362022280693\n",
      "Train Loss: 0.0584 | Val Loss: 0.4025\n",
      "Epoch 6/10\n",
      "0.028547124937176704\n",
      "0.020174581557512283\n",
      "0.049727220088243484\n",
      "0.04217163845896721\n",
      "0.043028365820646286\n",
      "0.08078908175230026\n",
      "0.03837420418858528\n",
      "0.04653215780854225\n",
      "0.1146242767572403\n",
      "0.08786354213953018\n",
      "0.0669105052947998\n",
      "0.026940150186419487\n",
      "0.06210716813802719\n",
      "0.013644153252243996\n",
      "0.004589653108268976\n",
      "0.07736962288618088\n",
      "0.12490832805633545\n",
      "0.06461650878190994\n",
      "0.0337219201028347\n",
      "0.019918404519557953\n",
      "0.07134123146533966\n",
      "0.02869987115263939\n",
      "0.011984664015471935\n",
      "0.07793815433979034\n",
      "0.004609337542206049\n",
      "0.006876496132463217\n",
      "0.0581674799323082\n",
      "0.039912015199661255\n",
      "0.03178318962454796\n",
      "0.07412049919366837\n",
      "0.038576774299144745\n",
      "0.057397231459617615\n",
      "0.006108367815613747\n",
      "0.0549025684595108\n",
      "0.013623015023767948\n",
      "0.09711923450231552\n",
      "0.02268630638718605\n",
      "0.025245117023587227\n",
      "0.08925598114728928\n",
      "0.040622781962156296\n",
      "0.02984277345240116\n",
      "0.0057753585278987885\n",
      "0.056261561810970306\n",
      "0.015151811763644218\n",
      "0.03968456760048866\n",
      "0.03292815014719963\n",
      "0.01401346456259489\n",
      "0.0740254670381546\n",
      "0.03114560805261135\n",
      "0.004863793961703777\n",
      "0.018814682960510254\n",
      "0.04125110059976578\n",
      "0.0043117087334394455\n",
      "0.09533317387104034\n",
      "0.028957512229681015\n",
      "0.03209347277879715\n",
      "0.019935399293899536\n",
      "0.03854632005095482\n",
      "0.009964647702872753\n",
      "0.013186353258788586\n",
      "0.03614386171102524\n",
      "0.009342605248093605\n",
      "0.08653423190116882\n",
      "0.08159145712852478\n",
      "0.07828986644744873\n",
      "0.01733108051121235\n",
      "0.03558751568198204\n",
      "0.040369387716054916\n",
      "0.12616321444511414\n",
      "0.03519681841135025\n",
      "0.011979243718087673\n",
      "0.03913326933979988\n",
      "0.05403159186244011\n",
      "0.14165599644184113\n",
      "0.045283615589141846\n",
      "0.020395195111632347\n",
      "0.018805602565407753\n",
      "0.057057950645685196\n",
      "1.601812800799962e-05\n",
      "Train Loss: 0.0435 | Val Loss: 0.3240\n",
      "Epoch 7/10\n",
      "0.010751403868198395\n",
      "0.00579823087900877\n",
      "0.010379095561802387\n",
      "0.01432760525494814\n",
      "0.0760812982916832\n",
      "0.05025719851255417\n",
      "0.017195090651512146\n",
      "0.028425300493836403\n",
      "0.16439953446388245\n",
      "0.007956398651003838\n",
      "0.025076260790228844\n",
      "0.00504261115565896\n",
      "0.02943314053118229\n",
      "0.045036863535642624\n",
      "0.003607031423598528\n",
      "0.011688237078487873\n",
      "0.026221012696623802\n",
      "0.009064750745892525\n",
      "0.07883483916521072\n",
      "0.02286818064749241\n",
      "0.017906617373228073\n",
      "0.012874210253357887\n",
      "0.009171480312943459\n",
      "0.03070339933037758\n",
      "0.013377871364355087\n",
      "0.06436129659414291\n",
      "0.008469007909297943\n",
      "0.022201215848326683\n",
      "0.06418599933385849\n",
      "0.014209965243935585\n",
      "0.0038672732189297676\n",
      "0.026293205097317696\n",
      "0.0302572138607502\n",
      "0.045715462416410446\n",
      "0.010223807767033577\n",
      "0.0157228484749794\n",
      "0.007824760861694813\n",
      "0.034741971641778946\n",
      "0.030114779248833656\n",
      "0.004506958648562431\n",
      "0.019695265218615532\n",
      "0.018341287970542908\n",
      "0.010910688899457455\n",
      "0.005203516688197851\n",
      "0.059054818004369736\n",
      "0.03738564997911453\n",
      "0.00843097548931837\n",
      "0.02610955759882927\n",
      "0.012192489579319954\n",
      "0.00469555938616395\n",
      "0.011301437392830849\n",
      "0.009018439799547195\n",
      "0.007073860615491867\n",
      "0.03383822739124298\n",
      "0.001869098749011755\n",
      "0.03170172497630119\n",
      "0.005492154508829117\n",
      "0.01697711832821369\n",
      "0.017152931541204453\n",
      "0.03382641077041626\n",
      "0.02363779954612255\n",
      "0.02238522656261921\n",
      "0.014544305391609669\n",
      "0.006122161168605089\n",
      "0.05481556057929993\n",
      "0.02143704891204834\n",
      "0.0870056301355362\n",
      "0.005216766614466906\n",
      "0.05751634016633034\n",
      "0.02808869071304798\n",
      "0.05793352797627449\n",
      "0.06100334972143173\n",
      "0.04065040126442909\n",
      "0.00447732163593173\n",
      "0.029150765389204025\n",
      "0.0046224407851696014\n",
      "0.023713335394859314\n",
      "0.0031803445890545845\n",
      "0.00023805872478988022\n",
      "Train Loss: 0.0256 | Val Loss: 0.3314\n",
      "Epoch 8/10\n",
      "0.004442460834980011\n",
      "0.030956074595451355\n",
      "0.004916248377412558\n",
      "0.007037082687020302\n",
      "0.007478560786694288\n",
      "0.011589780449867249\n",
      "0.012062382884323597\n",
      "0.0009267059504054487\n",
      "0.013560024090111256\n",
      "0.04311901330947876\n",
      "0.02009741961956024\n",
      "0.0005337328766472638\n",
      "0.0037360915448516607\n",
      "0.0013740456197410822\n",
      "0.0039329808205366135\n",
      "0.013874026015400887\n",
      "0.008961636573076248\n",
      "0.006378685589879751\n",
      "0.054109252989292145\n",
      "0.046997249126434326\n",
      "0.006725253537297249\n",
      "0.005443469621241093\n",
      "0.0029361166525632143\n",
      "0.003555876901373267\n",
      "0.002321264473721385\n",
      "0.005490878596901894\n",
      "0.0014923270791769028\n",
      "0.0011630068765953183\n",
      "0.007069556973874569\n",
      "0.02906784787774086\n",
      "0.003021962009370327\n",
      "0.005137193016707897\n",
      "0.014979585073888302\n",
      "0.010445792227983475\n",
      "0.04367070272564888\n",
      "0.0014872285537421703\n",
      "0.040281832218170166\n",
      "0.035934023559093475\n",
      "0.003389751072973013\n",
      "0.014575600624084473\n",
      "0.002684325445443392\n",
      "0.0052565219812095165\n",
      "0.00866827555000782\n",
      "0.010045912116765976\n",
      "0.09409622848033905\n",
      "0.04725770279765129\n",
      "0.01997268944978714\n",
      "0.05018562451004982\n",
      "0.005394144915044308\n",
      "0.0009362672572024167\n",
      "0.03521968796849251\n",
      "0.017411410808563232\n",
      "0.004620842169970274\n",
      "0.013995544984936714\n",
      "0.006571909878402948\n",
      "0.014574941247701645\n",
      "0.04207800328731537\n",
      "0.0199299193918705\n",
      "0.0028290501795709133\n",
      "0.009335685521364212\n",
      "0.013290135189890862\n",
      "0.0038974396884441376\n",
      "0.008849159814417362\n",
      "0.0050397166050970554\n",
      "0.008543469943106174\n",
      "0.003618215909227729\n",
      "0.011340804398059845\n",
      "0.01072266511619091\n",
      "0.028421645984053612\n",
      "0.06961189955472946\n",
      "0.00474373996257782\n",
      "0.001769836526364088\n",
      "0.0037077930755913258\n",
      "0.04033644497394562\n",
      "0.006230541504919529\n",
      "0.002791354898363352\n",
      "0.03031744621694088\n",
      "0.0030995323322713375\n",
      "0.0007003499194979668\n",
      "Train Loss: 0.0152 | Val Loss: 0.3951\n",
      "Epoch 9/10\n",
      "0.03125324100255966\n",
      "0.03402542322874069\n",
      "0.015981445088982582\n",
      "0.010715294629335403\n",
      "0.009591621346771717\n",
      "0.005537306424230337\n",
      "0.0018313097534701228\n",
      "0.0033469947520643473\n",
      "0.007510920986533165\n",
      "0.005721002351492643\n",
      "0.030144421383738518\n",
      "0.022642727941274643\n",
      "0.03144426271319389\n",
      "0.0020835446193814278\n",
      "0.0013148528523743153\n",
      "0.007040855009108782\n",
      "0.001737185288220644\n",
      "0.003108812728896737\n",
      "0.0617411844432354\n",
      "0.0009791514603421092\n",
      "0.001977143809199333\n",
      "0.013754505664110184\n",
      "0.00648370198905468\n",
      "0.003882739460095763\n",
      "0.005668874830007553\n",
      "0.0037044878117740154\n",
      "0.0009990439284592867\n",
      "0.007914306595921516\n",
      "0.01906253769993782\n",
      "0.021741678938269615\n",
      "0.005501373205333948\n",
      "0.030874930322170258\n",
      "0.006586454343050718\n",
      "0.002349559683352709\n",
      "0.0668458417057991\n",
      "0.0014656533021479845\n",
      "0.0068199364468455315\n",
      "0.004277357831597328\n",
      "0.006792878266423941\n",
      "0.04766671732068062\n",
      "0.0663624107837677\n",
      "0.014099911786615849\n",
      "0.02609911747276783\n",
      "0.016799094155430794\n",
      "0.009633688256144524\n",
      "0.04538576677441597\n",
      "0.009295620024204254\n",
      "0.062269579619169235\n",
      "0.002003380097448826\n",
      "0.0016117870109155774\n",
      "0.00847114808857441\n",
      "0.0352829284965992\n",
      "0.08201146125793457\n",
      "0.138941690325737\n",
      "0.025118201971054077\n",
      "0.01741081476211548\n",
      "0.025808174163103104\n",
      "0.008926140144467354\n",
      "0.0011818850180134177\n",
      "0.06670186668634415\n",
      "0.06788835674524307\n",
      "0.004423861857503653\n",
      "0.029006214812397957\n",
      "0.01751699484884739\n",
      "0.0135273402556777\n",
      "0.03546765074133873\n",
      "0.02222600020468235\n",
      "0.009378146380186081\n",
      "0.010607609525322914\n",
      "0.02212357148528099\n",
      "0.010361505672335625\n",
      "0.008547240868210793\n",
      "0.02523544803261757\n",
      "0.03922600299119949\n",
      "0.023742279037833214\n",
      "0.028440650552511215\n",
      "0.004902747925370932\n",
      "0.002576600993052125\n",
      "2.994608621520456e-05\n",
      "Train Loss: 0.0201 | Val Loss: 0.3636\n",
      "Epoch 10/10\n",
      "0.0023911460302770138\n",
      "0.0023918249644339085\n",
      "0.0011523379944264889\n",
      "0.007421979680657387\n",
      "0.08580805361270905\n",
      "0.02455052360892296\n",
      "0.0040899161249399185\n",
      "0.005422941409051418\n",
      "0.08441894501447678\n",
      "0.009369789622724056\n",
      "0.01007188018411398\n",
      "0.00034740156843326986\n",
      "0.001328598940744996\n",
      "0.0059120384976267815\n",
      "0.0015513021498918533\n",
      "0.03792857751250267\n",
      "0.036148253828287125\n",
      "0.030653223395347595\n",
      "0.06426405161619186\n",
      "0.00295018358156085\n",
      "0.01089538261294365\n",
      "0.002489191247150302\n",
      "0.004721585661172867\n",
      "0.021020544692873955\n",
      "0.001147683709859848\n",
      "0.031067805364727974\n",
      "0.050504256039857864\n",
      "0.02410716935992241\n",
      "0.06737391650676727\n",
      "0.002384327119216323\n",
      "0.0018986925715580583\n",
      "0.02844955399632454\n",
      "0.00820351205766201\n",
      "0.06630803644657135\n",
      "0.03778431564569473\n",
      "0.05180846527218819\n",
      "0.010798451490700245\n",
      "0.001981027889996767\n",
      "0.009069637395441532\n",
      "0.009384477511048317\n",
      "0.004198722541332245\n",
      "0.008200662210583687\n",
      "0.013321198523044586\n",
      "0.004677184857428074\n",
      "0.005078847520053387\n",
      "0.005035185720771551\n",
      "0.0008574015810154378\n",
      "0.001247671083547175\n",
      "0.010569182224571705\n",
      "0.025998961180448532\n",
      "0.01866966299712658\n",
      "0.027348006144165993\n",
      "0.012702136300504208\n",
      "0.010650012642145157\n",
      "0.008523251861333847\n",
      "0.05121782049536705\n",
      "0.006994062568992376\n",
      "0.007346024736762047\n",
      "0.007904889062047005\n",
      "0.014301340095698833\n",
      "0.0327143669128418\n",
      "0.04772613197565079\n",
      "0.005920303985476494\n",
      "0.011753569357097149\n",
      "0.04909185692667961\n",
      "0.0018750920426100492\n",
      "0.004417925141751766\n",
      "0.002732506487518549\n",
      "0.05073718726634979\n",
      "0.026876434683799744\n",
      "0.011127474717795849\n",
      "0.03556495159864426\n",
      "0.023134836927056313\n",
      "0.008890802972018719\n",
      "0.0030880176927894354\n",
      "0.002168782753869891\n",
      "0.005027616862207651\n",
      "0.002108291257172823\n",
      "2.1671266949851997e-05\n",
      "Train Loss: 0.0181 | Val Loss: 0.3489\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "num_train = train_x.shape[0]\n",
    "print(num_train)\n",
    "\n",
    "\n",
    "def train_epoch(images, labels):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    num_train = images.shape[0]\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        x = images[i:i+batch_size].to('cuda')\n",
    "        y = labels[i:i+batch_size].to('cuda')\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def validation_epoch(images, labels):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    num_samples = images.shape[0]\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        x = images[i:i+batch_size].to('cuda')\n",
    "        y = labels[i:i+batch_size].to('cuda')\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(x)\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    loss = train_epoch(images=train_x, labels=train_y)\n",
    "    val_loss = validation_epoch(test_x, test_y)\n",
    "    \n",
    "    print(f'Train Loss: {loss:0.4f} | Val Loss: {val_loss:0.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7648694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2-5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

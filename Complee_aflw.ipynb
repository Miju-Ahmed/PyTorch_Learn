{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2de97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# class AFLWDataset(Dataset):\n",
    "#     def __init__(self, image_paths, annotation_paths):\n",
    "#         self.image_paths = image_paths\n",
    "#         self.annotation_paths = annotation_paths\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.image_paths[idx]\n",
    "#         pts_path = self.annotation_paths[idx]\n",
    "\n",
    "#         image = cv2.imread(img_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         landmarks = self.read_pts(pts_path)\n",
    "\n",
    "#         return image, landmarks, img_path\n",
    "\n",
    "#     def read_pts(self, pts_path):\n",
    "#         coords = []\n",
    "#         with open(pts_path, 'r') as f:\n",
    "#             for line in f:\n",
    "#                 parts = line.strip().split()\n",
    "#                 if len(parts) == 3:\n",
    "#                     try:\n",
    "#                         x, y = float(parts[0]), float(parts[1])\n",
    "#                         vis = parts[2].lower() == 'true'\n",
    "#                         coords.append([x, y, vis])\n",
    "#                     except ValueError:\n",
    "#                         continue\n",
    "#         return np.array(coords).reshape(-1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19eeb2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "class AFLWDataset(Dataset):\n",
    "    def __init__(self, image_paths, pts_paths, image_size=(224, 224)):\n",
    "        self.image_paths = image_paths\n",
    "        self.pts_paths = pts_paths\n",
    "        self.image_size = image_size  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        pts_path = self.pts_paths[idx]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Original size\n",
    "        h0, w0 = image.shape[:2]\n",
    "\n",
    "        # Resize image\n",
    "        image = cv2.resize(image, self.image_size)\n",
    "\n",
    "        # Load landmarks and scale to new image size\n",
    "        landmarks = self.read_pts(pts_path)\n",
    "        landmarks = np.array(landmarks).reshape(-1, 3)\n",
    "        landmarks = landmarks[:, :2]  # Drop the visibility flag if present\n",
    "\n",
    "        # Scale landmarks\n",
    "        landmarks[:, 0] *= self.image_size[1] / w0\n",
    "        landmarks[:, 1] *= self.image_size[0] / h0\n",
    "\n",
    "        # Convert to tensor\n",
    "        image = torch.tensor(image).permute(2, 0, 1).float() / 255.0  # [C,H,W]\n",
    "        landmarks = torch.tensor(landmarks).float()  # [N,2]\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "    def read_pts(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        return [list(map(float, line.strip().split())) for line in lines if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3cda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def collect_aflw_paths(image_root, annotation_root):\n",
    "    image_paths = []\n",
    "    annotation_paths = []\n",
    "\n",
    "    for style_folder in sorted(os.listdir(image_root)):\n",
    "        style_path = os.path.join(image_root, style_folder)\n",
    "        if not os.path.isdir(style_path):\n",
    "            continue\n",
    "\n",
    "        for folder in sorted(os.listdir(style_path)):\n",
    "            img_folder = os.path.join(style_path, folder)\n",
    "            if not os.path.isdir(img_folder):\n",
    "                continue\n",
    "\n",
    "            for fname in os.listdir(img_folder):\n",
    "                if fname.endswith(\".jpg\"):\n",
    "                    img_path = os.path.join(img_folder, fname)\n",
    "                    ann_subdir = os.path.join(annotation_root, folder)\n",
    "                    if not os.path.exists(ann_subdir):\n",
    "                        continue\n",
    "\n",
    "                    pts_candidates = [f for f in os.listdir(ann_subdir) if f.startswith(fname[:-4])]\n",
    "                    if pts_candidates:\n",
    "                        pts_path = os.path.join(ann_subdir, pts_candidates[0])\n",
    "                        image_paths.append(img_path)\n",
    "                        annotation_paths.append(pts_path)\n",
    "\n",
    "    return image_paths, annotation_paths\n",
    "\n",
    "\n",
    "# âœ… Set your dataset path\n",
    "image_root = '/media/cse/ML/Miju_work/Dataset/AFLW-Style/AFLW-Style'\n",
    "annotation_root = os.path.join(image_root, 'annotation')\n",
    "\n",
    "# ðŸ“¦ Gather all image/annotation paths\n",
    "all_images, all_annotations = collect_aflw_paths(image_root, annotation_root)\n",
    "\n",
    "# ðŸ”€ Split into training and test sets\n",
    "train_imgs, test_imgs, train_anns, test_anns = train_test_split(\n",
    "    all_images, all_annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Create dataset objects\n",
    "train_dataset = AFLWDataset(train_imgs, train_anns)\n",
    "test_dataset = AFLWDataset(test_imgs, test_anns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef9fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0a15fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LandmarkModel(nn.Module):\n",
    "    def __init__(self, num_points=19):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, num_points * 2)  # (x,y) for each point\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80585f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, targets in dataloader:\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41409c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'True'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device)\u001b[39m\n\u001b[32m      6\u001b[39m model.train()\n\u001b[32m      7\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mAFLWDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     27\u001b[39m image = cv2.resize(image, \u001b[38;5;28mself\u001b[39m.image_size)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Load landmarks and scale to new image size\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m landmarks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_pts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m landmarks = np.array(landmarks).reshape(-\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m     32\u001b[39m landmarks = landmarks[:, :\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# Drop the visibility flag if present\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mAFLWDataset.read_pts\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     46\u001b[39m     lines = f.readlines()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines \u001b[38;5;28;01mif\u001b[39;00m line.strip()]\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'True'"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model setup\n",
    "model = LandmarkModel()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train\n",
    "for epoch in range(10):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

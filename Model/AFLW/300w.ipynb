{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c406596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34802afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Style300WDataset(Dataset):\n",
    "    def __init__(self, root_dir, variant='300W-Original', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to 300W-Style dataset (the folder containing all style folders).\n",
    "            variant (str): One of ['300W-Original', '300W-Gray', '300W-Light', '300W-Sketch'].\n",
    "            transform (callable): Transform to apply to images.\n",
    "        \"\"\"\n",
    "        self.image_dir = os.path.join(root_dir, variant)\n",
    "        self.anno_dir = os.path.join(root_dir, '300W-Annotations')\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for fname in os.listdir(self.image_dir):\n",
    "            if fname.endswith('.jpg') or fname.endswith('.png'):\n",
    "                image_path = os.path.join(self.image_dir, fname)\n",
    "                pts_name = os.path.splitext(fname)[0] + '.pts'\n",
    "                anno_path = os.path.join(self.anno_dir, pts_name)\n",
    "                if os.path.exists(anno_path):\n",
    "                    self.samples.append((image_path, anno_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _read_pts(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            coords = []\n",
    "            reading = False\n",
    "            for line in lines:\n",
    "                if line.strip() == '{':\n",
    "                    reading = True\n",
    "                    continue\n",
    "                if line.strip() == '}':\n",
    "                    break\n",
    "                if reading:\n",
    "                    x, y = map(float, line.strip().split())\n",
    "                    coords.append([x, y])\n",
    "        return np.array(coords, dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, pts_path = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        landmarks = self._read_pts(pts_path)\n",
    "        return image, torch.tensor(landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2ac84c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Style300WDataset(\n\u001b[1;32m     10\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/miju_chowdhury/Miju/Datasets/300W-Style/300W-Convert\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# <- change this to your actual path\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m300W-Sketch\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example: Loop through a batch\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, landmarks \u001b[38;5;129;01min\u001b[39;00m loader:\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/utils/data/dataloader.py:385\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 385\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/utils/data/sampler.py:156\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = Style300WDataset(\n",
    "    root_dir='/media/miju_chowdhury/Miju/Datasets/300W-Style/300W-Convert',   # <- change this to your actual path\n",
    "    variant='300W-Sketch',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Loop through a batch\n",
    "for images, landmarks in loader:\n",
    "    print(\"Image batch shape:\", images.shape)       # e.g. [32, 3, 224, 224]\n",
    "    print(\"Landmarks shape:\", landmarks.shape)      # e.g. [32, 68, 2]\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
